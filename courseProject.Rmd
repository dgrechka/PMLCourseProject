---
title: "Coursera Paractical Machine Learning Course Project"
author: "Dmitry A. Grechka"
date: "May 21, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Study design

1. Error rate definition
2. Splitting the data
3. Picking features
  + Using cross-validation
4. Picking prediction function
  + Using cross-validation
5. Evaluating out of sample error using test set

# Study

## Initial preparation

```{r data_loading,message=FALSE }
library(caret)
set.seed(12321)
barbell_lifts  <- read.csv('pml-training.csv')
```

## 1. Choosing error rate

Let's evaluate what kind of prediction we need

```{r}
str(barbell_lifts$classe)
```

We have the outcome variable `classe` to be a factor of 5.
This means that we need to do classification to assign observation to one of five classes. The appropriate error rate measure for this kind of classification is **accuracy** which accounts for false positives/negatives equally.

## 2. Splitting the data

To properly choose the training, testing, validation data set we need to evaluate the amount of available data.

```{r}
nrow(barbell_lifts)
```

We have thousands of observations in pml-training.csv which is large enough.
We can follow the following typical splitting scheme for large datases:

* 60% training
* 20% testing
* 20% validation

We will use testing dataset for refinement of features and prediction functions, validation dataset for final out-of-sample error evaluation

``` {r splitting_data}
inValidation <- createDataPartition(y=barbell_lifts$classe, p=0.2,list=F)

validation <- barbell_lifts[inValidation,]

nonValidation <- barbell_lifts[-inValidation,]
inTrain <- createDataPartition(y=nonValidation$classe, p=0.6 * 0.8,list=F)

training <- nonValidation[inTrain,]
testing <- nonValidation[-inTrain,]

```

## 3. Picking features

### 3.1. Clearing data

#### 3.1.1 Removing unrelated variables

The dataset contains several variables that we do not want to use as predictors

* user_name - we do not want to account a particular person doing dumbbell lifting
* cvtd_timestamp - we do not want to account the date and time of the dumbbell lifting
* X - we do not want to account dataset row number

```{r echo=F}
possPredictorNames <- colnames(training)

factorPredictorsNamesToOmit<- c('user_name','cvtd_timestamp','X','classe')

filteredPredicotrsNames <- c()

for (posPredName in possPredictorNames){
  if (posPredName %in% factorPredictorsNamesToOmit) {
    next # we are interested only in unrelated variables
  }
  
  filteredPredicotrsNames <- c(filteredPredicotrsNames,posPredName)
}

possPredictorNames <- filteredPredicotrsNames
```

After this filtering there are `r length(possPredictorNames)` possible predictors left.

#### 3.1.2 Converting factors with 'numeric' levels to real numeric
The data set contains several variables that are numeric, but treated as factors during import. Converting them to numeric

```{r echo=F, warning=F}

for (posPredName in possPredictorNames){
  if(class(training[[posPredName]]) == 'factor') {
    training[[posPredName]] <- as.numeric(as.character(training[[posPredName]])) #converting factor to cumeric via character
  }
}

training <- training[,names(training) %in% c('classe',filteredPredicotrsNames)]
```

#### 3.1.3 Identifing and elemination near zero variance predictors

```{r}
nzvars <- nearZeroVar(training)
training <- training[,-nzvars]
```

After this stage there are `r ncol(training)-1` presictors left

#### 3.1.4 Eliminating variables with lots of NA


Following possible predictors are excluded due to high NA portion

```{r echo=F}
naAllowedFraction <- .1

namesToCheck <- names(training)

for(curName in namesToCheck) {
  if(curName == 'classe')
    next;
  naFraction <- sum(is.na(training[[curName]]))/nrow(training)
  if(naFraction>naAllowedFraction) {
    print(paste("Fraction of NA in variable ",curName,' is ',naFraction,' which is more than ',naAllowedFraction,'.'))
    training <- training[,names(training) != curName]
  }
}
```

After this stage there are `r ncol(training)-1` presictors left

### 3.2 Transforming covariates

#### 3.2.1 Handling correlated predictors

Exploring the data set using correlation matrix plot from corrplot package.

```{r echo=F, fig.keep='none'}

predictorsOnly <- training[,names(training) != 'classe']

varM <- cor(predictorsOnly,use='complete.obs')
library(corrplot)
corrplot(varM,method = 'number',order='hclust')
```

```{r echo=F}

pcaGroup <- function(varaibleNames,groupName) {# side effect on training
  stopifnot(sum(names(training) %in% varaibleNames) == length(varaibleNames))
  correlatedGroup.i <- training[,names(training) %in% varaibleNames]
  pcaPreProc.i <- preProcess(correlatedGroup.i,method="pca")
  pcaGrp.i <- predict(pcaPreProc.i,newdata=training)
  
  #corM <- cor(correlatedGroup.i,use='complete.obs')
  #corrplot(corM,method = 'number',order='hclust')
  
  training <- training[,!(names(training) %in% varaibleNames)] #removing correlated variables
  for(i in 1:pcaPreProc.i$numComp) {
    training[[paste(groupName,'.PC',i,sep = '')]] <- pcaGrp.i[[paste('PC',i,sep = '')]]
  }
  list(pcaNum=pcaPreProc.i$numComp,predictFunc = pcaPreProc.i, transformedDS = training, groupName=groupName)
}
```

```{r echo=F}
#final check of covaraites
#corM <- cor(training[,names(training) != 'classe'],use='complete.obs')
#corrplot(corM,method = 'number',order='hclust')
```

```{r}
predictorsOnly <- training[,names(training) != 'classe']

pcaPreProc <- preProcess(predictorsOnly,method="pca")
pcaPredictedTraining <- predict(pcaPreProc,predictorsOnly)
pcaPredictedTraining$classe <- training$classe
```

## 4. Picking predictor function


```{r}
#testMask <- createDataPartition(pcaPredictedTraining$classe, p=0.1,list=F)
#pcaPredictedTrainingTrunc <- pcaPredictedTraining[testMask,]

rpartModel <- train(classe ~ .,data=pcaPredictedTraining,method="rpart")
rfModel <- train(classe ~ .,pcaPredictedTraining,method="rf")
gbmModel <- train(classe ~ .,pcaPredictedTraining,method="gbm")
ldaModel <- train(classe ~ .,pcaPredictedTraining,method="lda")
bayesModel <- train(classe ~ .,pcaPredictedTraining,method="nb")
svmModel <- train(classe ~ .,pcaPredictedTraining,method="svmRadial")

```

We will evaluate out-of-sample error for each of the predictor function by using testing posrtion of initial data.

```{r echo=F}

for (posPredName in possPredictorNames){
  if(class(testing[[posPredName]]) == 'factor') {
    testing[[posPredName]] <- as.numeric(as.character(testing[[posPredName]])) #converting factor to cumeric via character
  }
}

testing <- testing[,names(testing) %in% c('classe',possPredictorNames)]


testing.preproccessed <- applyPcaGroup(pcaGrp1,testing)
testing.preproccessed <- applyPcaGroup(pcaGrp2,testing.preproccessed)
testing.preproccessed <- applyPcaGroup(pcaGrp3,testing.preproccessed)
```

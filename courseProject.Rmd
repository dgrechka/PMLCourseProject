---
title: "Coursera Paractical Machine Learning Course Project"
author: "Dmitry A. Grechka"
date: "May 21, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Study design

1. Error rate definition
2. Splitting the data
3. Picking features
  + Clearing data: Removing unrelated variables
  + Clearing data: Converting factors with 'numeric' levels to real numeric
  + Clearing data: Identifing and elemination near zero variance predictors
  + Clearing data: Eliminating variables with lots of NA
  + Transforming covariates: Handling correlated predictors
4. Picking prediction function
  + Setting cross-validation options for parameter tuning
  + Tunning different functions
  + Ensembling final predictor
5. Evaluating out of sample error using test set

# Study

## Initial preparation

```{r data_loading,message=FALSE }
library(caret)
set.seed(12321)
barbell_lifts  <- read.csv('pml-training.csv')
```

## 1. Choosing error rate

Let's evaluate what kind of prediction we need

```{r}
str(barbell_lifts$classe)
```

We have the outcome variable `classe` to be a factor of 5.
This means that we need to do classification to assign observation to one of five classes. The appropriate error rate measure for this kind of classification is **accuracy** which accounts for false positives/negatives equally.

## 2. Splitting the data

To properly choose the training, testing, validation data set we need to evaluate the amount of available data.

```{r}
nrow(barbell_lifts)
```

We have thousands of observations in pml-training.csv which is large enough.
We can follow the following typical splitting scheme for large datases:

* 70% training
* 30% testing

We will use 10-fold  cross validation on training data set for prediction functions tuning. Testing dataset will be used for final out-of-sample error evaluation

``` {r splitting_data}
inTrain <- createDataPartition(y=barbell_lifts$classe, p=0.7,list=F)

training <- barbell_lifts[inTrain,]
testing <- barbell_lifts[-inTrain,]

```

## 3. Picking features

### 3.1. Clearing data

#### 3.1.1 Removing unrelated variables

The dataset contains several variables that we do not want to use as predictors

* user_name - we do not want to account a particular person doing dumbbell lifting
* cvtd_timestamp - we do not want to account the date and time of the dumbbell lifting
* X - we do not want to account dataset row number

```{r echo=F}
possPredictorNames <- colnames(training)

factorPredictorsNamesToOmit<- c('user_name','cvtd_timestamp','X','classe')

filteredPredicotrsNames <- c()

for (posPredName in possPredictorNames){
  if (posPredName %in% factorPredictorsNamesToOmit) {
    next # we are interested only in unrelated variables
  }
  
  filteredPredicotrsNames <- c(filteredPredicotrsNames,posPredName)
}

possPredictorNames <- filteredPredicotrsNames
```

After this filtering there are **`r length(possPredictorNames)`** possible predictors left.

#### 3.1.2 Converting factors with 'numeric' levels to real numeric
The data set contains several variables that are numeric, but treated as factors during import. Converting them to numeric

```{r echo=F, warning=F}

for (posPredName in possPredictorNames){
  if(class(training[[posPredName]]) == 'factor') {
    training[[posPredName]] <- as.numeric(as.character(training[[posPredName]])) #converting factor to cumeric via character
  }
}

training <- training[,names(training) %in% c('classe',filteredPredicotrsNames)]
```

#### 3.1.3 Identifing and elemination near zero variance predictors

```{r}
nzvars <- nearZeroVar(training)
training <- training[,-nzvars]
```

After this stage there are **`r ncol(training)-1`** presictors left

#### 3.1.4 Eliminating variables with lots of NA

```{r echo=F}
naAllowedFraction <- .1

namesToCheck <- names(training)

eliminatedDueNACount <- 0

for(curName in namesToCheck) {
  if(curName == 'classe')
    next;
  naFraction <- sum(is.na(training[[curName]]))/nrow(training)
  if(naFraction>naAllowedFraction) {
    #print(paste("Fraction of NA in variable ",curName,' is ',naFraction,' which is more than ',naAllowedFraction,'.'))
    eliminatedDueNACount <- eliminatedDueNACount+1
    training <- training[,names(training) != curName]
  }
}
```

After excluding the varaibles wich have more than 10% of NA values (there were `r eliminatedDueNACount` such varaibles) there are **`r ncol(training)-1`** presictors left

### 3.2 Transforming covariates:

#### Handling correlated predictors

Looking at the correlaction matrix of all `r ncol(training)-1` predictors

```{r}

predictorsOnly <- training[,names(training) != 'classe']
varM <- cor(predictorsOnly)
varMcol<- colorRampPalette(c("red", "white", "blue"))(20)
heatmap(x = varM, col = varMcol, symm = TRUE)

```

We can see that there are several correlated varaible clusters.

We will reduce predictor space dimensions by appling PCA.

```{r}
predictorsOnly <- training[,names(training) != 'classe']

pcaPreProc <- preProcess(predictorsOnly,method="pca")
pcaPredictedTraining <- predict(pcaPreProc,predictorsOnly)
pcaPredictedTraining$classe <- training$classe
```

After this stage we have **`r ncol(pcaPredictedTraining)-1`** linear independent principal component predictor.

## 4. Picking predictor function

### 4.1 Setting parameter tuning options

We will use 10-fold cross validation (as out dataset is large enough).

```{r}
trainOptions <- trainControl(
   method = "cv",
   number=10 #number of folds
)
```

### 4.2 Tring different prediction function families, tuning their parameters

We will use caretEnsemble package to traing different models with the same cross-validation options for parameters tuning

```{r}
library("caretEnsemble")

trained <- caretList(classe ~ .,
                     data = training,
                     trControl = trainOptions,
                     tuneList = list(
                       rpart_pca=caretModelSpec(method="rpart",preProcess="pca"),
                       #rf_pca=caretModelSpec(method="rf",preProcess="pca"),
                       #gbm_pca=caretModelSpec(method="gbm",preProcess="pca"),
                       lda_pca=caretModelSpec(method="lda",preProcess="pca"),
                       #nb_pca=caretModelSpec(method="nb",preProcess="pca"),
                       #svm_pca=caretModelSpec(method="svmRadial",preProcess="pca")
                       nnet=caretModelSpec(method="nnet")
                      )
                     )

#rpartModel <- train(classe ~ .,data=pcaPredictedTraining,method="rpart",tr)
#rfModel <- train(classe ~ .,pcaPredictedTraining,method="rf")
#gbmModel <- train(classe ~ .,pcaPredictedTraining,method="gbm")
#ldaModel <- train(classe ~ .,pcaPredictedTraining,method="lda")
#bayesModel <- train(classe ~ .,pcaPredictedTraining,method="nb")
#svmModel <- train(classe ~ .,pcaPredictedTraining,method="svmRadial")

```

We will evaluate out-of-sample error for each of the predictor function by using testing posrtion of initial data.

```{r echo=F}

for (posPredName in possPredictorNames){
  if(class(testing[[posPredName]]) == 'factor') {
    testing[[posPredName]] <- as.numeric(as.character(testing[[posPredName]])) #converting factor to cumeric via character
  }
}


```
